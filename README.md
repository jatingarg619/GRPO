# Phi-2 Quantized Inference and GRPO Training

This project implements:
1. 4-bit quantization for efficient inference using the Microsoft Phi-2 model
2. GRPO (Group Relative Policy Optimization) training for fine-tuning the model

## Setup

1. Create a virtual environment and activate it:
```bash
python -m venv venv
source venv/bin/activate  # On Windows, use: venv\Scripts\activate
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## Running Inference

The inference script (`inference.py`) implements:
- 4-bit quantization for efficient memory usage
- Interactive prompt-response interface
- Generation time tracking
- Automatic device mapping (CPU/GPU)

To start the inference:
```bash
python inference.py
```

## Training with GRPO

The training script (`train_grpo.py`) implements GRPO training as described in the [DeepSeekMath paper](https://huggingface.co/docs/trl/main/en/grpo_trainer). GRPO is an online learning algorithm that improves iteratively using data generated by the model during training.

### Training Setup

1. Ensure you have sufficient GPU memory (16GB+ recommended)

2. Install additional training dependencies if not already installed:
```bash
pip install trl>=0.7.11 datasets>=2.14.0 wandb peft>=0.7.0
```

3. Configure Weights & Biases (optional but recommended for tracking):
```bash
wandb login
```

### Running Training

1. Basic training command:
```bash
accelerate launch train_grpo.py
```

2. For multi-GPU training:
```bash
accelerate config  # Run this first to configure multi-GPU settings
accelerate launch train_grpo.py
```

### Training Process

The training script will:
1. Load the TLDR dataset for training
2. Initialize the Phi-2 model with GRPO configuration
3. Train for 3 epochs or 1000 steps (whichever comes first)
4. Save checkpoints every 100 steps in `phi2-grpo-output`
5. Save the final model in `phi2-grpo-final`

### Reward Function

The model uses a custom reward function that encourages optimal response lengths:
- Responses under 100 characters: Reward increases linearly
- Responses between 100-200 characters: Maximum reward (1.0)
- Responses over 200 characters: Gradually decreasing reward

### Training Features:
- Uses the TLDR dataset for training
- Custom reward function based on completion length
- Mixed precision training (FP16)
- Automatic device mapping
- Wandb logging integration

### Training Parameters:
- Learning rate: 1e-5
- Batch size: 4 per device
- Gradient accumulation steps: 2
- Training epochs: 3
- Max steps: 1000
- Warmup steps: 100

### GRPO Specific Parameters:
- Number of generations per prompt: 8
- Max prompt length: 512
- Max completion length: 200
- Temperature: 0.7
- Top-p: 0.9
- Beta (KL coefficient): 0.04

### Monitoring Training

1. Console Output:
- Training progress is logged every 10 steps
- Sample completions are printed every 100 steps
- Loss and reward metrics are continuously updated

2. Weights & Biases (if configured):
- Real-time training metrics
- Generated completions
- Model checkpoints
- System resource usage

### Resuming Training

To resume from a checkpoint:
```bash
accelerate launch train_grpo.py --resume_from_checkpoint phi2-grpo-output/checkpoint-{step}
```

### Using the Trained Model

After training completes:
1. The model will be saved in `phi2-grpo-final`
2. To use the trained model for inference, update the model path in `inference.py`:
```python
model_name = "phi2-grpo-final"  # Instead of "microsoft/phi-2"
```

## Example Outputs

Here are some example outputs from the model:

### 1. Poetry Generation
**Prompt:** Write a short poem about artificial intelligence:

**Response:**
```
Artificial intelligence, a world of its own,
Where machines learn and grow,
From algorithms to neural networks,
A future that's bright and full of tricks.
```
*Generation time: 28.54 seconds*

### 2. Technical Explanation
**Prompt:** Explain quantum computing in simple terms:

**Response:**
```
Quantum computing is a type of computing that uses the principles of quantum mechanics to perform calculations. Unlike classical computers, which use bits to store and process information, quantum computers use quantum bits, or qubits, which can exist in multiple states at the same time. This allows quantum computers to perform certain calculations much faster than classical computers.

Some key points:
1. Qubits can exist in multiple states simultaneously
2. Quantum computers excel at parallel calculations
3. Practical applications include factoring large numbers for cryptography
4. Challenges include maintaining qubit coherence and high costs
```
*Generation time: 20.52 seconds*

### 3. Code Generation
**Prompt:** Write a function to calculate fibonacci numbers in Python:

**Response:**
```python
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)

# Optimized version with memoization
def fibonacci_memo(n, memo={}):
    if n in memo:
        return memo[n]
    elif n <= 1:
        result = n
    else:
        result = fibonacci(n-1, memo) + fibonacci(n-2, memo)
    memo[n] = result
    return result

# Dynamic programming version
def fibonacci_dp(n):
    fib_values = [0, 1] + [0]*(n-1)
    for i in range(2, n+1):
        fib_values[i] = fib_values[i-1] + fib_values[i-2]
    return fib_values[n]
```

## Model Details

The implementation uses:
- Base model: Microsoft Phi-2 (2.7B parameters)
- Quantization: 4-bit precision
- Mixed precision: FP16 for compute
- Memory efficient: Requires significantly less VRAM than full precision

## System Requirements

- GPU with at least 6GB VRAM (for 4-bit quantization)
- Python 3.8 or higher
- CUDA-compatible GPU (recommended)
- For training: At least 16GB VRAM recommended

## Notes

- The model uses automatic device mapping and will use GPU if available
- Generation parameters can be adjusted in the script for different response characteristics
- Training logs and checkpoints are saved in the `phi2-grpo-output` directory
- Final trained model is saved in `phi2-grpo-final` # GRPO
